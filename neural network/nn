import numpy as np
import os
import pandas as pd

class Neural_Network:
    def __init__(self,**kwargs):
        self._network = []
        self._inputs = kwargs['inputs']
        self._testing_data = None
        self._target_values = kwargs['targets']
        self._learning_rate = kwargs['learning_rate']
        self._output_percentages = []

        activation_function = input("Which activation function do you wish to use?\n")
        for i in range(kwargs['no_layers']):
            if i == 0:
                try:
                    #For datasets only
                    inputs=len(self._inputs.columns.values)
                except:
                    inputs = len(self._inputs)
            else:
                inputs = self._network[-1].get_neurons()
            neurons = int(input("How many neurons do you want in layer #" + str(i+1) + "?\n"))
            self._network.append(Layer_Dense(no_inputs=inputs,no_neurons=neurons,activation_function=activation_function))
        outputs = int(input("How many outputs do you wish to have in your neural network?\n"))
        inputs = self._network[-1].get_neurons()
        self._network.append(Layer_Dense(no_inputs=inputs,no_neurons=outputs,activation_function="softmax"))

    def structure(self):
        structure = []
        for layer in self._network:
            structure.append(layer.get_neurons())
        print("The structure of this neural network is:", structure)

    def run(self, **kwargs):
        epochs = kwargs['epochs']
        #Start by putting initial inputs into the input layer and generating the network
        self._network[0].forward(self._inputs)
        for i in range(len(self._network)-1):
            #Using the previous layer's outputs as the next layer's inputs
            self._network[i+1].forward(self._network[i].output)

        for i in range(epochs):
            #Forward pass
            self._network[0].forward_pass(self._inputs)
            for i in range(len(self._network)-1):
                output = self._network[i+1].forward_pass(self._network[i].output)

            #Generates the values for loss function, used for training in multiple passes
            #Backbone of backpropagation
            loss = neural.evaluate()

            #Backward pass
            #Somehow find a way to derive the evalaute function on predicted values and target values
            error, confidences = [np.e**-x for x in loss]
            confidences = [np.e**-x for x in confidences]
            error = confidences

            for i in range(len(self._network)-1,-1):
                error = self._network[i-1].backward(error, self._learning_rate)
                print('Epoch %d/%d' % (i+1, epochs))

        #Start by putting initial inputs into the input layer
        self._network[0].forward(self._testing_data)
        for i in range(len(self._network)-1):
            #Using the previous layer's outputs as the next layer's inputs
            self._network[i+1].forward(self._network[i].output)
        print("The network's testing outputs were:", self._network[-1].output)

    def train_test_split(self):
        #Split the available dataset into training data and testing data
        split = float(input("Enter a decimal between 0 and 1 to represent the fraction of data to be used for training:\n"))
        assert split >= 0 and split <=1
        data_split = round(split * len(self._inputs))
        training_data = self._inputs.iloc[0:data_split]
        testing_data = self._inputs.iloc[data_split:]
        self._inputs = training_data
        #At time of editing, this data set is empty. Why?
        self._testing_data = testing_data

    def evaluate(self):
        #Target values are the y values that we want to be predicting correctly

        #You can calculate the loss of a categorical neural network (basically most NN) by using
        #categorical cross-entropy

        #Using one-hot encoding to calculate the categorical cross-entropy of data (loss)
        #In one-hot encoding, we assign the target class position we want in our array of outputs
        #Then make an array of 0s of the same length as outputs but put a 1 in the target class position
        #This basically simplifies to just the negative natural logarithm of the predicted target value

        #The following code will represent the confidence values in the predictions made by the NN
        #For this to work, if categorical, the number of outputs must equal the number of possible class targets
        #E.g for gender, there's two possible class targets (0 and 1), so two output neurons
        #The string can be changed to the attribute in the table that you shall be predicting

        #A short but ugly way of getting a start to complete this task
        '''
        loss = -np.log(self._network[-1].output[range(len(self._network[-1].output)),target_values.loc[:,"gender"]])
        average_loss = np.mean(loss)
        '''

        #A nicer way to accomplish the same thing
        samples = len(self._network[-1].output)
        #Clip the values so we don't get any infinity errors if a confidence level happens to be spot on
        y_pred_clipped = np.clip(self._network[-1].output, 1e-7, 1-1e-7)

        #If one-hot encoding has not been passed in
        if len(self._target_values.shape) == 1:
            #Selecting the largest confidences based on their position
            correct_confidences = y_pred_clipped[range(samples),self._target_values[:samples]]
        elif len(self._target_values.shape) == 2:
            #One-hot encoding has been used in this scenario
            correct_confidences = np.sum(y_pred_clipped*self._target_values[:samples], axis=1)

        #Calculate the loss and return
        loss = -np.log(correct_confidences)
        return loss, correct_confidences

class GetStuff():
    def __init__(cls):
        pass

    def get_directory(cls,file_name):
        for root, dirs, files in os.walk(r'/'):
            for name in files:
                if name == file_name:
                    return os.path.abspath(os.path.join(root, name))

class Layer_Dense:
    def __init__(self, **kwargs):
        self._neurons = kwargs['no_neurons']
        self._activation_function = kwargs.get('activation_function')
        #Original line which works with normal array
        self.weights = np.random.randn(kwargs['no_inputs'], self._neurons)

        #Biases are a 1D array, so 1 list of a length of the number of neurons
        self.biases = np.zeros((1, self._neurons))

    def get_neurons(self):
        return self._neurons

    def forward(self, inputs):
        self.inputs = inputs
        #OUtput unaffected by the activation function prior to any extra bedazzle
        self._ideal_output = np.dot(inputs, self.weights) + self.biases
        #OUtput affected by the activation function
        self.output = []
        for layer_output in self._ideal_output:
            self.output.append([])
            for neuron_output in layer_output:
                #trigger the activation function

                if self._activation_function.lower() == 'relu':
                    neuron_output = np.maximum(0,neuron_output)
                elif self._activation_function.lower() == 'sigmoid':
                    neuron_output = 1 / (1 + np.exp(-neuron_output))
                #These section of code needs to be sorted out pronto - It seems to be adding another dimension to the data
                elif self._activation_function.lower() == 'softmax':
                    #Exponentiate (e to the power of x) values and subtract largest value of layer to prevent overflow
                    #Afterwards, normalise (put as relative fractions) the output values
                    #In theory, to get the max value out of each batch, axis should be set to 1 and keepdims should be True
                    neuron_output = np.exp(neuron_output - np.max(layer_output,axis=0)) / np.sum(np.exp(layer_output),axis=0)
                self.output[-1].append(neuron_output)

    #Taken from above but used to prevent overaddition of data to the network outputs
    #Used for the forward pass during the training of the neural network
    def forward_pass(self, inputs):
        self.inputs = inputs
        #OUtput unaffected by the activation function prior to any extra bedazzle
        self._ideal_output = np.dot(inputs, self.weights) + self.biases
        return self._ideal_output

    def backward(self, output_error, learning_rate):
        #The error of this layer's inputs is equal to its output error multipled by the 
        #transposed weights of the layer
        input_error = np.dot(output_error, self.weights.T)
        #The error of the weights in this layer is equal to the transposed matrix of inputs fed into the layer
        #multipled by the error of the output from this layer
        weights_error = np.dot(self.inputs.T, output_error)
        # dBias = output_error

        # update parameters
        self.weights -= learning_rate * weights_error
        self.biases -= learning_rate * output_error
        return input_error


#First, I'll need to fetch the inputs from a pre-processed dataset which I've created earlier
input_directory = GetStuff().get_directory("dataset.csv")
data = pd.read_csv(input_directory, engine="python")
network_inputs = data

#Then, fetch the data with the values that I'm meant to be predicting
target_directory = GetStuff().get_directory("starting_data.csv")
target_values = pd.read_csv(target_directory, engine="python")

#I'm currently creating a neural network with an input layer and 3 hidden layers
neural = Neural_Network(inputs=network_inputs, learning_rate=0.3, no_layers=4, targets=target_values['gender'])
#Prints the structure of the network
neural.structure()
neural.train_test_split()
neural.run(epochs=500)